{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Author: <font color=\"blue\">Miguel Rando Ramirez</font>\n",
    "\n",
    "E-mail: <font color=\"blue\">miguel.rando01@estudiant.upf.edu</font>\n",
    "\n",
    "Date: <font color=\"blue\">26/11/2024</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import io\n",
    "import nltk\n",
    "import gzip\n",
    "import random\n",
    "import statistics\n",
    "import secrets\n",
    "import re\n",
    "import gzip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 0. Dataset and how to iterate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "INPUT_FILE = \"movie_lines.tsv.gz\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "POS_NOUN = 'NN'\n",
    "POS_VERB = 'VB'\n",
    "POS_ADJECTIVE = 'JJ'\n",
    "\n",
    "# Producer in Python that reads a file by words that are nouns\n",
    "def read_by_parts_of_speech(filename, parts_of_speech, max_words=-1, report_every=-1):\n",
    "    \n",
    "    # Open the input file\n",
    "    with gzip.open(INPUT_FILE, \"rt\", encoding='utf8') as file:\n",
    "        \n",
    "        # Initialize counter of words to stop at max_words\n",
    "        counter = 0\n",
    "    \n",
    "        # Iterate through lines in the file\n",
    "        for line in file:\n",
    "            \n",
    "            elements = line.split(\"\\t\")\n",
    "            \n",
    "            text = \"\"\n",
    "            if len(elements) >= 5:\n",
    "                text = elements[4].strip()\n",
    "                                        \n",
    "            if counter > max_words and max_words != -1:\n",
    "                break\n",
    "                \n",
    "            for sentence in nltk.sent_tokenize(text):\n",
    "                \n",
    "                tagged = nltk.pos_tag(nltk.word_tokenize(sentence))\n",
    "                for word in [part[0] for part in tagged if part[1] in parts_of_speech]:\n",
    "                \n",
    "                    counter += 1\n",
    "\n",
    "                    # Report\n",
    "                    if (report_every != -1) and (counter % report_every == 0):\n",
    "                        if max_words == -1:\n",
    "                            print(\"- Read %d words so far\" % (counter))\n",
    "                        else:\n",
    "                            print(\"- Read %d/%d words so far\" % (counter, max_words))\n",
    "\n",
    "                    # Produce the word in lowercase\n",
    "                    yield word.lower()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package punkt to /Users/miguel/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /Users/miguel/nltk_data...\n",
      "[nltk_data]   Unzipping taggers/averaged_perceptron_tagger.zip.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "nltk.download('punkt')\n",
    "nltk.download('averaged_perceptron_tagger')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Current noun 'same'\n",
      "Current noun 'ta'\n",
      "Current noun 'few'\n",
      "Current noun 'such'\n",
      "Current noun 'dear'\n",
      "Current noun 'many'\n",
      "Current noun 'liberty'\n",
      "Current noun 'right'\n",
      "Current noun 'numb'\n",
      "Current noun 'good'\n",
      "Current noun 'schizophrenic'\n",
      "Current noun 'few'\n",
      "Current noun 'many'\n",
      "Current noun 'dumb'\n",
      "- Read 10000/30000 words so far\n",
      "Current noun 'detective'\n",
      "Current noun 'unknown'\n",
      "Current noun 'eventual'\n",
      "Current noun 'regrettable'\n",
      "Current noun 'glad'\n",
      "Current noun 'future'\n",
      "Current noun 'short'\n",
      "Current noun 'british'\n",
      "Current noun 'new'\n",
      "Current noun 'first'\n",
      "- Read 20000/30000 words so far\n",
      "Current noun 'favorite'\n",
      "Current noun 'warm'\n",
      "Current noun 'good'\n",
      "Current noun 'real'\n",
      "Current noun 'easy'\n",
      "Current noun 'rich'\n",
      "Current noun 'guilt'\n",
      "- Read 30000/30000 words so far\n"
     ]
    }
   ],
   "source": [
    "for word in read_by_parts_of_speech(INPUT_FILE, [POS_ADJECTIVE], max_words=30000, report_every=10000):\n",
    "    # Prints 1/1000 of words\n",
    "    if random.random() < 0.001:\n",
    "        print(\"Current noun '%s'\" % (word)) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 1. Determine approximately the top-10 words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"add_reservoir\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def add_to_reservoir(reservoir, item, max_reservoir_size):\n",
    "    # YOUR CODE HERE\n",
    "    \n",
    "    if len(reservoir) < max_reservoir_size:\n",
    "        # If the reservoir is not full we add the item\n",
    "        reservoir.append(item)\n",
    "    else:\n",
    "        random_index = random.randint(0, max_reservoir_size - 1)\n",
    "        reservoir[random_index] = item\n",
    "\n",
    "    \n",
    "    assert(len(reservoir) <= max_reservoir_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code for \"reservoir_sampling\"</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def reservoir_sampling(filename, parts_of_speech, reservoir_size, max_words=-1, report_every=-1):\n",
    "    reservoir = []\n",
    "\n",
    "    words_read = 0\n",
    "\n",
    "    for word in read_by_parts_of_speech(filename, parts_of_speech, max_words=max_words, report_every=report_every):\n",
    "\n",
    "            # YOUR CODE HERE\n",
    "            \n",
    "        words_read += 1\n",
    "\n",
    "        if len(reservoir) < reservoir_size:\n",
    "            reservoir.append(word)\n",
    "        else:\n",
    "            probability = reservoir_size / words_read\n",
    "            if random.random() < probability:\n",
    "                # replace a random element in the reservoir\n",
    "                add_to_reservoir(reservoir, word, reservoir_size)\n",
    "\n",
    "    return (words_read, reservoir)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Number of items seen    : 30001\n",
      "Number of items sampled : 1500\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "reservoir_size = 1500\n",
    "(items_seen, reservoir) = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=30000, report_every=10000)\n",
    "\n",
    "print(\"Number of items seen    : %d\" % items_seen)\n",
    "print(\"Number of items sampled : %d\" % len(reservoir) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "51 good\n",
      "40 little\n",
      "29 last\n",
      "27 sure\n",
      "27 much\n",
      "25 right\n",
      "24 other\n",
      "23 dead\n",
      "22 real\n",
      "20 sorry\n",
      "19 bad\n",
      "17 old\n",
      "17 many\n",
      "16 wrong\n",
      "16 ready\n",
      "16 great\n",
      "16 big\n",
      "15 next\n",
      "15 new\n",
      "14 first\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "freq = {}\n",
    "for item in reservoir:\n",
    "    freq[item] = reservoir.count(item)\n",
    "\n",
    "most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:20]\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    print(\"%d %s\" % (absolute_frequency, word))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to print the top items and their relative frequencies</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Top 20 most frequent items and their relative frequencies:\n",
      "51 (3.40%) good\n",
      "40 (2.67%) little\n",
      "29 (1.93%) last\n",
      "27 (1.80%) sure\n",
      "27 (1.80%) much\n",
      "25 (1.67%) right\n",
      "24 (1.60%) other\n",
      "23 (1.53%) dead\n",
      "22 (1.47%) real\n",
      "20 (1.33%) sorry\n",
      "19 (1.27%) bad\n",
      "17 (1.13%) old\n",
      "17 (1.13%) many\n",
      "16 (1.07%) wrong\n",
      "16 (1.07%) ready\n",
      "16 (1.07%) great\n",
      "16 (1.07%) big\n",
      "15 (1.00%) next\n",
      "15 (1.00%) new\n",
      "14 (0.93%) first\n"
     ]
    }
   ],
   "source": [
    "total_items = len(reservoir)\n",
    "\n",
    "# Print absolute and relative frequencies\n",
    "print(\"Top 20 most frequent items and their relative frequencies:\")\n",
    "for absolute_frequency, word in most_frequent_items:\n",
    "    relative_frequency = (absolute_frequency / total_items) * 100\n",
    "    print(f\"{absolute_frequency} ({relative_frequency:.2f}%) {word}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Increase the max limit of words so that one pass takes about 2-3 minutes to be completed. Replace this cell with your code to try different reservoir sizes. In each case, print your estimate for the relative and absolute frequency of the words in the entire dataset.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Reservoir size: 50\n",
      "- Read 10000/50000 words so far\n",
      "- Read 20000/50000 words so far\n",
      "- Read 30000/50000 words so far\n",
      "- Read 40000/50000 words so far\n",
      "- Read 50000/50000 words so far\n",
      "Top 5 words in the reservoir:\n",
      "Word: uh, Absolute Frequency: 2, Estimated Frequency: 2000.00\n",
      "Word: real, Absolute Frequency: 2, Estimated Frequency: 2000.00\n",
      "Word: many, Absolute Frequency: 2, Estimated Frequency: 2000.00\n",
      "Word: last, Absolute Frequency: 2, Estimated Frequency: 2000.00\n",
      "Word: hard, Absolute Frequency: 2, Estimated Frequency: 2000.00\n",
      "\n",
      "Reservoir size: 100\n",
      "- Read 10000/50000 words so far\n",
      "- Read 20000/50000 words so far\n",
      "- Read 30000/50000 words so far\n",
      "- Read 40000/50000 words so far\n",
      "- Read 50000/50000 words so far\n",
      "Top 5 words in the reservoir:\n",
      "Word: good, Absolute Frequency: 6, Estimated Frequency: 3000.00\n",
      "Word: right, Absolute Frequency: 3, Estimated Frequency: 1500.00\n",
      "Word: little, Absolute Frequency: 3, Estimated Frequency: 1500.00\n",
      "Word: hundred, Absolute Frequency: 3, Estimated Frequency: 1500.00\n",
      "Word: own, Absolute Frequency: 2, Estimated Frequency: 1000.00\n",
      "\n",
      "Reservoir size: 500\n",
      "- Read 10000/50000 words so far\n",
      "- Read 20000/50000 words so far\n",
      "- Read 30000/50000 words so far\n",
      "- Read 40000/50000 words so far\n",
      "- Read 50000/50000 words so far\n",
      "Top 5 words in the reservoir:\n",
      "Word: good, Absolute Frequency: 19, Estimated Frequency: 1900.00\n",
      "Word: little, Absolute Frequency: 14, Estimated Frequency: 1400.00\n",
      "Word: sure, Absolute Frequency: 11, Estimated Frequency: 1100.00\n",
      "Word: last, Absolute Frequency: 10, Estimated Frequency: 1000.00\n",
      "Word: new, Absolute Frequency: 9, Estimated Frequency: 900.00\n",
      "\n",
      "Reservoir size: 1000\n",
      "- Read 10000/50000 words so far\n",
      "- Read 20000/50000 words so far\n",
      "- Read 30000/50000 words so far\n",
      "- Read 40000/50000 words so far\n",
      "- Read 50000/50000 words so far\n",
      "Top 5 words in the reservoir:\n",
      "Word: good, Absolute Frequency: 51, Estimated Frequency: 2550.00\n",
      "Word: little, Absolute Frequency: 28, Estimated Frequency: 1400.00\n",
      "Word: other, Absolute Frequency: 26, Estimated Frequency: 1300.00\n",
      "Word: right, Absolute Frequency: 25, Estimated Frequency: 1250.00\n",
      "Word: sure, Absolute Frequency: 18, Estimated Frequency: 900.00\n"
     ]
    }
   ],
   "source": [
    "max_words = 50000  # This value should be chosen so that the processing takes about 2-3 minutes\n",
    "\n",
    "reservoir_sizes = [50, 100, 500, 1000]\n",
    "\n",
    "\n",
    "# Loop over different  sizes\n",
    "for reservoir_size in reservoir_sizes:\n",
    "    print(f\"\\nReservoir size: {reservoir_size}\")\n",
    "    \n",
    "    items_seen, reservoir = reservoir_sampling(INPUT_FILE, [POS_ADJECTIVE], reservoir_size, max_words=max_words, report_every=10000)\n",
    "    \n",
    "    freq = {}\n",
    "    for item in reservoir:\n",
    "        freq[item] = reservoir.count(item)\n",
    "    \n",
    "    most_frequent_items = sorted([(frequency, word) for word, frequency in freq.items()], reverse=True)[:5]\n",
    "    \n",
    "    # Print top 5 words \n",
    "    print(\"Top 5 words in the reservoir:\")\n",
    "    for absolute_frequency, word in most_frequent_items:\n",
    "        estimated_frequency = absolute_frequency * max_words / reservoir_size\n",
    "        print(f\"Word: {word}, Absolute Frequency: {absolute_frequency}, Estimated Frequency: {estimated_frequency:.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with a brief commentary indicating what reservoir size you would recommend to use, and your overall conclusions.</font>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Larger reservoirs (for example 1000) have more of the diversity, giving more stable top-3 results.\n",
    "\n",
    "Size 1000 gives me back the same 3 words few times (good, little, other). But i suposse than even a bigger size would be better."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 2. Determine approximately the distinct number of words"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def count_trailing_zeroes(number):\n",
    "    count = 0\n",
    "    while number & 1 == 0:\n",
    "        count += 1\n",
    "        number = number >> 1\n",
    "    return count"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "def random_hash_function():\n",
    "    # We use a cryptographically safe generator for the salt of our hash function\n",
    "    salt = secrets.token_bytes(32)\n",
    "    return lambda string: hash(string + str(salt))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Replace this cell with your code to perform the requested number of passes.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 1: 2048 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 2: 4096 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 3: 4096 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 4: 16384 distinct words\n",
      "- Read 10000/30000 words so far\n",
      "- Read 20000/30000 words so far\n",
      "- Read 30000/30000 words so far\n",
      "Estimate on pass 5: 8192 distinct words\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 5\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    hash_function = random_hash_function()\n",
    "\n",
    "    max_trailing_zeroes = 0\n",
    "\n",
    "    for word in read_by_parts_of_speech(INPUT_FILE, [POS_ADJECTIVE], max_words= 30000, report_every= 10000):\n",
    "        \n",
    "        # Calcular el valor del hash \n",
    "        hash_value = hash_function(word)\n",
    "        \n",
    "        # Contar los ceros \n",
    "        trailing_zeroes = count_trailing_zeroes(hash_value)\n",
    "\n",
    "        max_trailing_zeroes = max(max_trailing_zeroes, trailing_zeroes)\n",
    "\n",
    "    # Estimar el número de palabras distintas \n",
    "    estimate = 2 ** max_trailing_zeroes\n",
    "    \n",
    "    \n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 6963.2\n",
      "* Median  of estimates: 4096.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+1\" color=\"red\">Compute the median of average estimates in 3 separate runs of your algorithm; each run should do 10 passes over the file. Repeat this for nouns (POS_NOUN), adjectives (POS_ADJECTIVE), and verbs (POS_VERB). Replace this cell with the results you obtained in each pass, and whether the average or the median seem more appropriate for this probabilistic counting.</font>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 1: 1024 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 2: 8192 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 3: 2048 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 4: 2048 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 5: 16384 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 6: 16384 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 7: 4096 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 8: 16384 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 9: 4096 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 10: 32768 distinct words\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 10\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    hash_function = random_hash_function()\n",
    "\n",
    "    max_trailing_zeroes = 0\n",
    "\n",
    "    for word in read_by_parts_of_speech(INPUT_FILE, [POS_ADJECTIVE], max_words= 10000, report_every= 10000):\n",
    "        \n",
    "        # Calcular el valor del hash \n",
    "        hash_value = hash_function(word)\n",
    "        \n",
    "        # Contar los ceros \n",
    "        trailing_zeroes = count_trailing_zeroes(hash_value)\n",
    "\n",
    "        max_trailing_zeroes = max(max_trailing_zeroes, trailing_zeroes)\n",
    "\n",
    "    # Estimar el número de palabras distintas \n",
    "    estimate = 2 ** max_trailing_zeroes\n",
    "    \n",
    "    \n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 10342.4\n",
      "* Median  of estimates: 6144.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 1: 262144 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 2: 2048 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 3: 8192 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 4: 8192 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 5: 1024 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 6: 1024 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 7: 1024 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 8: 4096 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 9: 16384 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 10: 8192 distinct words\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 10\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    hash_function = random_hash_function()\n",
    "\n",
    "    max_trailing_zeroes = 0\n",
    "\n",
    "    for word in read_by_parts_of_speech(INPUT_FILE, [POS_NOUN], max_words= 10000, report_every= 10000):\n",
    "        \n",
    "        # Calcular el valor del hash \n",
    "        hash_value = hash_function(word)\n",
    "        \n",
    "        # Contar los ceros \n",
    "        trailing_zeroes = count_trailing_zeroes(hash_value)\n",
    "\n",
    "        max_trailing_zeroes = max(max_trailing_zeroes, trailing_zeroes)\n",
    "\n",
    "    # Estimar el número de palabras distintas \n",
    "    estimate = 2 ** max_trailing_zeroes\n",
    "    \n",
    "    \n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 31232.0\n",
      "* Median  of estimates: 6144.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 1: 1024 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 2: 512 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 3: 1024 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 4: 1024 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 5: 512 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 6: 1024 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 7: 2048 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 8: 1024 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 9: 256 distinct words\n",
      "- Read 10000/10000 words so far\n",
      "Estimate on pass 10: 512 distinct words\n"
     ]
    }
   ],
   "source": [
    "number_of_passes = 10\n",
    "estimates = []\n",
    "\n",
    "for i in range(number_of_passes):\n",
    "    # YOUR_CODE_HERE: read the file and generate an estimate\n",
    "    hash_function = random_hash_function()\n",
    "\n",
    "    max_trailing_zeroes = 0\n",
    "\n",
    "    for word in read_by_parts_of_speech(INPUT_FILE, [POS_VERB], max_words= 10000, report_every= 10000):\n",
    "        \n",
    "        # Calcular el valor del hash \n",
    "        hash_value = hash_function(word)\n",
    "        \n",
    "        # Contar los ceros \n",
    "        trailing_zeroes = count_trailing_zeroes(hash_value)\n",
    "\n",
    "        max_trailing_zeroes = max(max_trailing_zeroes, trailing_zeroes)\n",
    "\n",
    "    # Estimar el número de palabras distintas \n",
    "    estimate = 2 ** max_trailing_zeroes\n",
    "    \n",
    "    \n",
    "    \n",
    "    estimates.append(estimate)\n",
    "    print(\"Estimate on pass %d: %d distinct words\" % (i+1, estimate))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "* Average of estimates: 896.0\n",
      "* Median  of estimates: 1024.0\n"
     ]
    }
   ],
   "source": [
    "# Leave this code as-is\n",
    "\n",
    "print(\"* Average of estimates: %.1f\" % statistics.mean(estimates))\n",
    "print(\"* Median  of estimates: %.1f\" % statistics.median(estimates))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<font size=\"+2\" color=\"#003300\">I hereby declare that, except for the code provided by the course instructors, all of my code, report, and figures were produced by myself.</font>"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
